# Model Configuration
model:
  name: "modern_llm"
  vocab_size: 32000
  hidden_size: 4096
  num_layers: 32
  num_heads: 32
  num_kv_heads: 8  # Multi-Query Attention
  intermediate_size: 11008
  max_position_embeddings: 4096
  rope_theta: 10000.0
  use_cache: true
  
  # Modern architecture choices
  hidden_act: "silu"  # SwiGLU activation
  rms_norm_eps: 1e-6  # RMSNorm instead of LayerNorm
  tie_word_embeddings: false
  use_rotary_emb: true
  use_flash_attention: true
  
  # Mixture of Experts (optional)
  use_moe: false
  num_experts: 8
  num_experts_per_tok: 2

# Training Configuration
training:
  batch_size: 4
  gradient_accumulation_steps: 16
  learning_rate: 3e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  max_grad_norm: 1.0
  warmup_steps: 2000
  max_steps: 100000
  
  # Modern optimization
  optimizer: "adamw"
  lr_scheduler: "cosine"
  use_mixed_precision: true
  gradient_checkpointing: true
  
  # Logging and checkpointing
  logging_steps: 10
  save_steps: 1000
  eval_steps: 500
  max_checkpoints: 3

# Data Configuration
data:
  dataset_name: "openwebtext"
  sequence_length: 2048
  num_proc: 8
  streaming: false
  
  # Tokenizer
  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  add_special_tokens: true

# Evaluation
evaluation:
  eval_dataset: "wikitext"
  eval_batch_size: 8
  perplexity_eval: true
  generate_samples: true
  num_samples: 10

# Infrastructure
system:
  device: "auto"  # auto-detect GPU
  distributed: false
  num_gpus: 1
  mixed_precision: "bf16"
  compile_model: true  # PyTorch 2.0 compilation
  
# Experiment tracking
wandb:
  project: "modern-llm"
  entity: null
  name: null
  tags: ["transformer", "llm", "2025"]
